{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- product_price: double (nullable = true)\n",
      " |-- product_description: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- payment_amount: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- transaction_date: date (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      "\n",
      "+--------------------+------------+----------------+-------------+--------------------+\n",
      "|          product_id|product_name|product_category|product_price| product_description|\n",
      "+--------------------+------------+----------------+-------------+--------------------+\n",
      "|1d2fd6dc-403f-427...|        baby|     Electronics|       157.74|Third develop guy...|\n",
      "|6b615674-ee9f-44c...|       order|        Clothing|       714.54|Collection up for...|\n",
      "|aa8192f6-9eed-40d...|     believe|     Electronics|       550.49|Admit ball war sh...|\n",
      "|1ffd70aa-3e7b-4c8...|        trip|     Electronics|        801.9|Understand if the...|\n",
      "|6ce3e2a9-8533-4bf...|     history|           Books|       208.92|Travel she alone ...|\n",
      "+--------------------+------------+----------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+--------------------+-------------------+--------------+--------------+----------------+---------+--------------------+\n",
      "|      transaction_id|         customer_id|      customer_name|payment_amount|payment_method|transaction_date|  country|          product_id|\n",
      "+--------------------+--------------------+-------------------+--------------+--------------+----------------+---------+--------------------+\n",
      "|32afda36-d1c6-4e4...|cc464fa7-50fe-4c9...|       Amanda Clark|        107.62|   Credit Card|      2024-10-03|   Norway|c49343e8-2214-41f...|\n",
      "|5a8d405e-781c-405...|9494844a-7933-4e8...|     Amanda Coleman|        970.56|   Credit Card|      2024-06-07|     Mali|dee40e28-3003-47e...|\n",
      "|0e2e5190-fece-491...|3e4363a0-ef40-42b...|Rebecca Johnson DDS|        118.34|   Credit Card|      2024-06-09| Mongolia|f039a8c3-2e66-433...|\n",
      "|8324220d-a5b9-4bc...|7aff7737-0f2f-46b...|     Noah Olsen Jr.|        437.59|   Credit Card|      2024-03-10|   Canada|da8738ab-667c-426...|\n",
      "|66534e95-77fd-4f3...|6542d274-d5a3-484...|    James Zimmerman|         287.9|   Credit Card|      2024-01-05|Nicaragua|fdd4171b-e3ed-486...|\n",
      "+--------------------+--------------------+-------------------+--------------+--------------+----------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, when # pyspark sql functions\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType, TimestampType, DecimalType # pyspark sql types\n",
    "import random\n",
    "from faker import Faker\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# SPARK SETUP\n",
    "spark = SparkSession.builder.appName(\"PySpark Synthetic\").getOrCreate() # Create a SparkSession, which is the entry point to any Spark functionality\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Generate synthetic product data, this would be the raw data, extract phase, ingestion phase.\n",
    "# can be replaced with data warehouse or data lake creation credentials which would means we skip the extract phase\n",
    "def generate_synthetic_product_data(num_records=1000):\n",
    "    data = []\n",
    "    product_ids = [fake.uuid4() for _ in range(num_records)]\n",
    "    for product_id in product_ids:\n",
    "        data.append({\n",
    "            'product_id': product_id,\n",
    "            'product_name': fake.word(),\n",
    "            'product_category': random.choice(['Electronics', 'Clothing', 'Books', 'Home & Kitchen']),\n",
    "            'product_price': round(random.uniform(10.0, 1000.0), 2),\n",
    "            'product_description': fake.sentence(),\n",
    "        })\n",
    "    return data, product_ids\n",
    "\n",
    "# Generate synthetic payment data\n",
    "def generate_synthetic_payment_data(num_records=1000, product_ids=None):\n",
    "    if product_ids is None or len(product_ids) < num_records:\n",
    "        raise ValueError(\"Not enough product IDs provided\")\n",
    "    \n",
    "    data = []\n",
    "    for _ in range(num_records): \n",
    "        data.append({\n",
    "            'transaction_id': fake.uuid4(),\n",
    "            'customer_id': fake.uuid4(),\n",
    "            'customer_name': fake.name(),\n",
    "            'payment_amount': round(random.uniform(10.0, 1000.0), 2),\n",
    "            'payment_method': random.choice(['Credit Card', 'Debit Card', 'PayPal', 'Bank Transfer']),\n",
    "            'transaction_date': fake.date_this_year(),\n",
    "            'country': fake.country(),\n",
    "            'product_id': random.choice(product_ids),\n",
    "        })\n",
    "    return data\n",
    "\n",
    "# Generate 1000 records of product data first\n",
    "product_data, product_ids = generate_synthetic_product_data(1000)\n",
    "\n",
    "# Convert product data to Pandas DataFrame before converting to Spark DataFrame\n",
    "product_df = pd.DataFrame(product_data)\n",
    "# Convert to Spark DataFrame\n",
    "spark_product_df = spark.createDataFrame(product_df)\n",
    "\n",
    "# Generate 1000 records of payment data using the product IDs\n",
    "payment_data = generate_synthetic_payment_data(1000, product_ids)\n",
    "\n",
    "# Convert payment data to Pandas DataFrame before converting to Spark DataFrame\n",
    "payment_df = pd.DataFrame(payment_data)\n",
    "# Convert to Spark DataFrame\n",
    "spark_payment_df = spark.createDataFrame(payment_df)\n",
    "\n",
    "# Repartition the DataFrames so that Spark can handle them efficiently\n",
    "spark_product_df = spark_product_df.repartition(4)\n",
    "spark_payment_df = spark_payment_df.repartition(4)\n",
    "\n",
    "# Display schemas of both DataFrames for verification\n",
    "spark_product_df.printSchema()\n",
    "spark_payment_df.printSchema()\n",
    "\n",
    "# Show the first few rows of both DataFrames for verification\n",
    "spark_product_df.show(5)\n",
    "spark_payment_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 records\n",
    "display(spark_payment_df.limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('transaction_id', 'string'),\n",
       " ('customer_id', 'string'),\n",
       " ('customer_name', 'string'),\n",
       " ('payment_amount', 'double'),\n",
       " ('payment_method', 'string'),\n",
       " ('transaction_date', 'date'),\n",
       " ('country', 'string'),\n",
       " ('product_id', 'string')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data types\n",
    "spark_payment_df.dtypes\n",
    "# double is a float in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature Engineering\n",
    "from pyspark.sql.functions import year, month, dayofweek, datediff, current_date, when, col, avg, sum, row_number, desc\n",
    "from pyspark.sql.window import Window # Window is used for window functions and we need lag function for days since last transaction\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "# 1. Extract time-based features. for each row, extract the year, month, and day of the week\n",
    "spark_payment_df = spark_payment_df.withColumn(\"transaction_year\", year(\"transaction_date\"))\n",
    "spark_payment_df = spark_payment_df.withColumn(\"transaction_month\", month(\"transaction_date\"))\n",
    "spark_payment_df = spark_payment_df.withColumn(\"transaction_day_of_week\", dayofweek(\"transaction_date\"))\n",
    "\n",
    "# 2. Calculate days since last transaction (for each customer), so we can see if the customer is active or not\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(\"transaction_date\") # Window specification for the window function\n",
    "spark_payment_df = spark_payment_df.withColumn(\"days_since_last_transaction\", \n",
    "                                               datediff(\"transaction_date\", lag(\"transaction_date\").over(window_spec))) # lag is the previous row by partition via window specification\n",
    "\n",
    "# 3. Create a binary feature for high-value transactions, so we can see if the transaction is a high value transaction\n",
    "avg_payment = spark_payment_df.select(avg(\"payment_amount\")).first()[0]\n",
    "spark_payment_df = spark_payment_df.withColumn(\"is_high_value_transaction\", \n",
    "                                               when(col(\"payment_amount\") > avg_payment, 1).otherwise(0))\n",
    "\n",
    "# 4. Calculate customer lifetime value (CLV), this is the total amount of money a customer will spend on the business, it implies customer loyalty\n",
    "clv_df = spark_payment_df.groupBy(\"customer_id\").agg(sum(\"payment_amount\").alias(\"customer_lifetime_value\"))\n",
    "spark_payment_df = spark_payment_df.join(clv_df, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "# 5. Create a binary feature for preferred payment method, this is to see if the customer is using their preferred payment method\n",
    "# we extract it by grouping the payment method by customer and ordering it by the count of the payment method, then we take the first row, which gives us the most used payment method\n",
    "preferred_payment_method = spark_payment_df.groupBy(\"customer_id\", \"payment_method\").count() \\\n",
    "                                           .withColumn(\"rank\", row_number().over(Window.partitionBy(\"customer_id\").orderBy(desc(\"count\")))) \\\n",
    "                                           .filter(col(\"rank\") == 1) \\\n",
    "                                           .select(\"customer_id\", \"payment_method\").withColumnRenamed(\"payment_method\", \"preferred_payment_method\")\n",
    "spark_payment_df = spark_payment_df.join(preferred_payment_method, on=\"customer_id\", how=\"left\") \\\n",
    "                                   .withColumn(\"is_preferred_payment_method\", \n",
    "                                               when(col(\"payment_method\") == col(\"preferred_payment_method\"), 1).otherwise(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip time from transaction_date\n",
    "spark_payment_df = spark_payment_df.withColumn(\"transaction_date\", col(\"transaction_date\").cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|transaction_date|\n",
      "+----------------+\n",
      "|      2024-05-12|\n",
      "|      2024-08-05|\n",
      "|      2024-02-27|\n",
      "|      2024-06-22|\n",
      "|      2024-07-31|\n",
      "|      2024-09-23|\n",
      "|      2024-10-12|\n",
      "|      2024-06-15|\n",
      "|      2024-01-09|\n",
      "|      2024-01-11|\n",
      "|      2024-04-27|\n",
      "|      2024-07-07|\n",
      "|      2024-03-01|\n",
      "|      2024-04-29|\n",
      "|      2024-08-14|\n",
      "|      2024-07-08|\n",
      "|      2024-07-26|\n",
      "|      2024-10-03|\n",
      "|      2024-03-10|\n",
      "|      2024-06-25|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check values in tx date\n",
    "spark_payment_df.select(\"transaction_date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----------------+-------------+--------------------+\n",
      "|          product_id|product_name|product_category|product_price| product_description|\n",
      "+--------------------+------------+----------------+-------------+--------------------+\n",
      "|1d2fd6dc-403f-427...|        baby|     Electronics|       157.74|Third develop guy...|\n",
      "|6b615674-ee9f-44c...|       order|        Clothing|       714.54|Collection up for...|\n",
      "|aa8192f6-9eed-40d...|     believe|     Electronics|       550.49|Admit ball war sh...|\n",
      "|1ffd70aa-3e7b-4c8...|        trip|     Electronics|        801.9|Understand if the...|\n",
      "|6ce3e2a9-8533-4bf...|     history|           Books|       208.92|Travel she alone ...|\n",
      "|beda3173-208e-440...|     feeling|  Home & Kitchen|       231.41|Manager lawyer po...|\n",
      "|8c480eb5-dda6-4b3...|        talk|     Electronics|       519.46|Wind including di...|\n",
      "|0ca3b4c2-791c-453...|        thus|        Clothing|        123.5|Discuss picture o...|\n",
      "|e86883ca-d671-4af...|      myself|           Books|       418.79| Tax lead guess set.|\n",
      "|aa74218d-dc42-4f7...|         try|     Electronics|       944.01|Watch week in nev...|\n",
      "|e284d286-46ea-451...|    specific|  Home & Kitchen|        294.7|Less parent leg c...|\n",
      "|27d9c84d-e2e9-4d0...|      action|           Books|        14.74|Represent free is...|\n",
      "|c7bc9d28-7b74-4cd...|      matter|        Clothing|        138.9|Evidence though b...|\n",
      "|e69b0f98-a54d-453...|     explain|     Electronics|       611.77|Technology increa...|\n",
      "|a48b7146-305d-4b7...|      report|     Electronics|       523.91|Tv practice Ameri...|\n",
      "|9b752bca-5173-4f7...|      nearly|  Home & Kitchen|       120.64|Chair trial happe...|\n",
      "|e833ddd9-97fe-4b0...|    actually|           Books|       438.57|    Child fly store.|\n",
      "|96b8d997-a8bf-4ee...|       might|     Electronics|       137.29|Food commercial d...|\n",
      "|dec2f36f-7c45-4ac...|       peace|        Clothing|       920.76|Relate control ce...|\n",
      "|12a36a2d-8847-44a...|        hard|  Home & Kitchen|        37.82|Beautiful else la...|\n",
      "+--------------------+------------+----------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check product data\n",
    "spark_product_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
