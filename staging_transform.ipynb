{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, count, sum, avg, max, min, stddev, mean, udf, round\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofweek\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"StagingTransform\").getOrCreate()\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "spark_payment_df = spark.read.parquet('s3://bucket-name/path/to/output/payment/') # or redshift\n",
    "\n",
    "# same for product_df\n",
    "spark_product_df = spark.read.parquet('s3://bucket-name/path/to/output/payment/') # or redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "# Extract time-based features. for each row, extract the year, month, and day of the week\n",
    "spark_payment_df = spark_payment_df.withColumn(\"transaction_year\", year(\"transaction_date\"))\n",
    "spark_payment_df = spark_payment_df.withColumn(\"transaction_month\", month(\"transaction_date\"))\n",
    "spark_payment_df = spark_payment_df.withColumn(\"transaction_day_of_week\", dayofweek(\"transaction_date\"))\n",
    "\n",
    "# Calculate days since last transaction (for each customer), so we can see if the customer is active or not\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(\"transaction_date\") # Window specification for the window function\n",
    "spark_payment_df = spark_payment_df.withColumn(\"days_since_last_transaction\", \n",
    "                                               datediff(\"transaction_date\", lag(\"transaction_date\").over(window_spec))) # lag is the previous row by partition via window specification\n",
    "\n",
    "# Create a binary feature for high-value transactions, so we can see if the transaction is a high value transaction\n",
    "avg_payment = spark_payment_df.select(avg(\"payment_amount\")).first()[0]\n",
    "spark_payment_df = spark_payment_df.withColumn(\"is_high_value_transaction\", \n",
    "                                               when(col(\"payment_amount\") > avg_payment, 1).otherwise(0))\n",
    "\n",
    "# Calculate customer lifetime value (CLV), this is the total amount of money a customer will spend on the business, it implies customer loyalty\n",
    "clv_df = spark_payment_df.groupBy(\"customer_id\").agg(sum(\"payment_amount\").alias(\"customer_lifetime_value\"))\n",
    "spark_payment_df = spark_payment_df.join(clv_df, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "# Create a binary feature for preferred payment method, this is to see if the customer is using their preferred payment method\n",
    "# we extract it by grouping the payment method by customer and ordering it by the count of the payment method, then we take the first row, which gives us the most used payment method\n",
    "preferred_payment_method = spark_payment_df.groupBy(\"customer_id\", \"payment_method\").count() \\\n",
    "                                           .withColumn(\"rank\", row_number().over(Window.partitionBy(\"customer_id\").orderBy(desc(\"count\")))) \\\n",
    "                                           .filter(col(\"rank\") == 1) \\\n",
    "                                           .select(\"customer_id\", \"payment_method\").withColumnRenamed(\"payment_method\", \"preferred_payment_method\")\n",
    "spark_payment_df = spark_payment_df.join(preferred_payment_method, on=\"customer_id\", how=\"left\") \\\n",
    "                                   .withColumn(\"is_preferred_payment_method\", \n",
    "                                               when(col(\"payment_method\") == col(\"preferred_payment_method\"), 1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip time from transaction_date\n",
    "spark_payment_df = spark_payment_df.withColumn(\"transaction_date\", col(\"transaction_date\").cast(DateType()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
