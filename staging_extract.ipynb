{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, when # pyspark sql functions\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType, TimestampType, DecimalType # pyspark sql types\n",
    "import random\n",
    "from faker import Faker\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# SPARK SETUP for AWS GLUE\n",
    "#    #hive enables for spark to use catalog\n",
    "#    #spark.hadoop.fs.s3.impl is required to use s3a:// paths, s3a is more efficient and suitable for accesing s3 in aws glue\n",
    "#    #spark.serializer is required to use KryoSerializer, it is faster than default serializer\n",
    "#    #spark.sql.catalogImplementation is required to use hive catalog, to track metadata\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AWS Glue PySpark Synthetic\") \\\n",
    "    .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate() # Create a SparkSession, which is the entry point to any Spark functionality\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Generate synthetic product data, this would be the raw data, extract phase, ingestion phase.\n",
    "# can be replaced with data warehouse or data lake creation credentials\n",
    "def generate_synthetic_product_data(num_records=1000):\n",
    "    data = []\n",
    "    product_ids = [fake.uuid4() for _ in range(num_records)]\n",
    "    for product_id in product_ids:\n",
    "        data.append({\n",
    "            'product_id': product_id,\n",
    "            'product_name': fake.word(),\n",
    "            'product_category': random.choice(['Electronics', 'Clothing', 'Books', 'Home & Kitchen']),\n",
    "            'product_price': round(random.uniform(10.0, 1000.0), 2),\n",
    "            'product_description': fake.sentence(),\n",
    "        })\n",
    "    return data, product_ids\n",
    "\n",
    "# Generate synthetic payment data\n",
    "def generate_synthetic_payment_data(num_records=1000, product_ids=None):\n",
    "    if product_ids is None or len(product_ids) < num_records:\n",
    "        raise ValueError(\"Not enough product IDs provided\")\n",
    "    \n",
    "    data = []\n",
    "    for _ in range(num_records): \n",
    "        data.append({\n",
    "            'transaction_id': fake.uuid4(),\n",
    "            'customer_id': fake.uuid4(),\n",
    "            'customer_name': fake.name(),\n",
    "            'payment_amount': round(random.uniform(10.0, 1000.0), 2),\n",
    "            'payment_method': random.choice(['Credit Card', 'Debit Card', 'PayPal', 'Bank Transfer']),\n",
    "            'transaction_date': fake.date_this_year(),\n",
    "            'country': fake.country(),\n",
    "            'product_id': random.choice(product_ids),\n",
    "        })\n",
    "    return data\n",
    "\n",
    "# Generate 1000 records of product data first\n",
    "product_data, product_ids = generate_synthetic_product_data(1000)\n",
    "\n",
    "# Convert product data to Pandas DataFrame before converting to Spark DataFrame\n",
    "product_df = pd.DataFrame(product_data)\n",
    "# Convert to Spark DataFrame\n",
    "spark_product_df = spark.createDataFrame(product_df)\n",
    "\n",
    "# Write product data to S3\n",
    "# spark_product_df.write.mode('overwrite').parquet('s3://your-bucket-name/path/to/output/product/')\n",
    "\n",
    "# Generate 1000 records of payment data using the product IDs\n",
    "payment_data = generate_synthetic_payment_data(1000, product_ids)\n",
    "\n",
    "# Convert payment data to Pandas DataFrame before converting to Spark DataFrame\n",
    "payment_df = pd.DataFrame(payment_data)\n",
    "# Convert to Spark DataFrame\n",
    "spark_payment_df = spark.createDataFrame(payment_df)\n",
    "\n",
    "# Write payment data to S3 in Parquet format\n",
    "# spark_payment_df.write.mode('overwrite').parquet('s3://your-bucket-name/path/to/output/payment/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert product data to Pandas DataFrame before converting to Spark DataFrame\n",
    "product_df = pd.DataFrame(product_data)\n",
    "# Convert to Spark DataFrame\n",
    "spark_product_df = spark.createDataFrame(product_df)\n",
    "\n",
    "# Convert payment data to Pandas DataFrame before converting to Spark DataFrame\n",
    "payment_df = pd.DataFrame(payment_data)\n",
    "# Convert to Spark DataFrame\n",
    "spark_payment_df = spark.createDataFrame(payment_df)\n",
    "\n",
    "# Repartition the DataFrames so that Spark can handle them efficiently\n",
    "spark_product_df = spark_product_df.repartition(4)\n",
    "spark_payment_df = spark_payment_df.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('transaction_id', 'string'),\n",
       " ('customer_id', 'string'),\n",
       " ('customer_name', 'string'),\n",
       " ('payment_amount', 'double'),\n",
       " ('payment_method', 'string'),\n",
       " ('transaction_date', 'date'),\n",
       " ('country', 'string'),\n",
       " ('product_id', 'string')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data types\n",
    "spark_payment_df.dtypes\n",
    "# double is a float in pyspark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
